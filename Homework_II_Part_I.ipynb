{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e3a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note Grakel does not seem to support Python >=3.10, Python 3.9 works fine\n",
    "# you are free to remove imports that are not useful for you\n",
    "\n",
    "\n",
    "from grakel.datasets import fetch_dataset\n",
    "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import KernelPCA # to check your own implementation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MUTAG dataset consists of 188 chemical compounds divided into two\n",
    "classes according to their mutagenic effect on a bacterium.\n",
    "\n",
    "The chemical data was obtained form http://cdb.ics.uci.edu and converted\n",
    "to graphs, where vertices represent atoms and edges represent chemical\n",
    "bonds. Explicit hydrogen atoms have been removed and vertices are labeled\n",
    "by atom type and edges by bond type (single, double, triple or aromatic).\n",
    "Chemical data was processed using the Chemistry Development Kit (v1.4).\n",
    "\n",
    "ENZYMES is a dataset of protein tertiary structures obtained from (Borgwardt et al., 2005)\n",
    "consisting of 600 enzymes from the BRENDA enzyme database (Schomburg et al., 2004).\n",
    "In this case the task is to correctly assign each enzyme to one of the 6 EC top-level\n",
    "classes.\n",
    "\n",
    "NCI1 and NCI109 represent two balanced subsets of datasets of chemical compounds screened\n",
    "for activity against non-small cell lung cancer and ovarian cancer cell lines respectively\n",
    "(Wale and Karypis (2006) and http://pubchem.ncbi.nlm.nih.gov).\n",
    "\n",
    "More datasets here https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The grakel training datasets are a list of graphs, where for each graph, we have its corresponding adjacency set (aka the set of pairs of node that are connected to each other), a dictionary that maps each node integer to its label value (which also happens to be an int), and another dictionary that maps edges to labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195a11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdjacencySet = Set[Tuple[int, int]] # Set of adjacent node pairs\n",
    "Edge = Tuple[int, int] # Type of a graph edge\n",
    "dataset_MUT = fetch_dataset(\"MUTAG\", verbose=False)\n",
    "\n",
    "G_MUT: List[Tuple[AdjacencySet, Dict[int, int], Dict[Edge, int]]] = dataset_MUT.data\n",
    "y_MUT: np.ndarray = dataset_MUT.target\n",
    "\n",
    "dataset_ENZ = fetch_dataset(\"ENZYMES\", verbose=False)\n",
    "G_ENZ: List[Tuple[AdjacencySet, Dict[int, int], Dict[Edge, int]]] = dataset_ENZ.data\n",
    "y_ENZ: np.ndarray = dataset_ENZ.target\n",
    "\n",
    "dataset_NCI = fetch_dataset(\"NCI1\", verbose=False)\n",
    "G_NCI: List[Tuple[AdjacencySet, Dict[int, int], Dict[Edge, int]]] = dataset_NCI.data\n",
    "y_NCI: np.ndarray = dataset_NCI.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "type(graph)=<class 'set'>\n",
      "{(3, 4), (4, 3), (5, 4), (12, 13), (5, 7), (14, 13), (8, 9), (9, 8), (9, 14), (17, 15), (10, 9), (1, 6), (13, 14), (6, 5), (15, 17), (4, 5), (14, 9), (5, 6), (9, 10), (1, 2), (10, 11), (2, 1), (11, 10), (6, 1), (15, 13), (15, 16), (13, 15), (16, 15), (3, 2), (12, 11), (4, 10), (8, 7), (10, 4), (2, 3), (11, 12), (13, 12), (7, 5), (7, 8)}\n",
      "type(graph)=<class 'dict'>\n",
      "{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 1, 16: 2, 17: 2}\n",
      "type(graph)=<class 'dict'>\n",
      "{(2, 1): 0, (1, 2): 0, (3, 2): 0, (2, 3): 0, (4, 3): 0, (3, 4): 0, (5, 4): 0, (4, 5): 0, (6, 5): 0, (5, 6): 0, (6, 1): 0, (1, 6): 0, (7, 5): 0, (5, 7): 0, (8, 7): 0, (7, 8): 0, (9, 8): 0, (8, 9): 0, (10, 9): 0, (9, 10): 0, (10, 4): 0, (4, 10): 0, (11, 10): 0, (10, 11): 0, (12, 11): 0, (11, 12): 0, (13, 12): 0, (12, 13): 0, (14, 13): 0, (13, 14): 0, (14, 9): 0, (9, 14): 0, (15, 13): 1, (13, 15): 1, (16, 15): 2, (15, 16): 2, (17, 15): 1, (15, 17): 1}\n"
     ]
    }
   ],
   "source": [
    "print(type(G_MUT[0]))\n",
    "for graph_metadata in (G_MUT[0]):\n",
    "    print(f\"{type(graph_metadata)=}\")\n",
    "    print(graph_metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1, -1, -1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1, -1,\n        1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1, -1,  1, -1, -1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1, -1, -1,  1,  1,\n        1, -1,  1,  1, -1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1, -1,  1,\n        1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1,  1,  1, -1,\n       -1,  1,  1, -1, -1,  1,  1,  1,  1, -1, -1, -1, -1, -1,  1, -1,  1,\n        1, -1, -1,  1, -1, -1, -1, -1,  1,  1, -1,  1,  1, -1,  1,  1,  1,\n       -1, -1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n        1,  1,  1,  1, -1,  1,  1,  1, -1,  1, -1, -1,  1,  1, -1, -1,  1,\n       -1])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_MUT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.1. Weisfeiler-Lehman subtree complexity\n",
    "See report."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.2. Compute the kernels\n",
    "The arbitrary number of iterations $H$ that we whose is 10, so that it coincides with the question 1.4..\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db4cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weis_leh = WeisfeilerLehman(n_iter=10, base_graph_kernel=VertexHistogram)\n",
    "K_MUT: np.ndarray = weis_leh.fit_transform(G_MUT)\n",
    "K_ENZ: np.ndarray = weis_leh.fit_transform(G_ENZ)\n",
    "K_NCI: np.ndarray = weis_leh.fit_transform(G_NCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f843cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel.shape=(188, 188)\n",
      "[[507 210 206 ... 189 473 289]\n",
      " [210 263 145 ... 126 260 181]\n",
      " [206 145 263 ... 129 256 186]\n",
      " ...\n",
      " [189 126 129 ... 228 231 179]\n",
      " [473 260 256 ... 231 859 361]\n",
      " [289 181 186 ... 179 361 396]]\n",
      "kernel.shape=(600, 600)\n",
      "[[1279.  502.  585. ...  973.  998.  911.]\n",
      " [ 502.  569.  362. ...  591.  615.  565.]\n",
      " [ 585.  362.  791. ...  587.  678.  617.]\n",
      " ...\n",
      " [ 973.  591.  587. ... 2439. 1417. 1333.]\n",
      " [ 998.  615.  678. ... 1417. 1953. 1423.]\n",
      " [ 911.  565.  617. ... 1333. 1423. 1834.]]\n",
      "kernel.shape=(4110, 4110)\n",
      "[[ 467  267  333 ...  455  397  424]\n",
      " [ 267  718  288 ...  755  671  652]\n",
      " [ 333  288  935 ...  482  429  446]\n",
      " ...\n",
      " [ 455  755  482 ... 1929 1231 1216]\n",
      " [ 397  671  429 ... 1231 1402 1020]\n",
      " [ 424  652  446 ... 1216 1020 1673]]\n"
     ]
    }
   ],
   "source": [
    "for kernel in (K_MUT, K_ENZ, K_NCI):\n",
    "    print(f\"{kernel.shape=}\")\n",
    "    print(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.4. Explicit embedding versus kernel\n",
    "Computing the rank of the WL subtree kernel matrix for all three datasets, with H=10 iterations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a2fc9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\tRank\t# samples\n",
      "MUTAG\t175\t188\n",
      "ENZYMES\t595\t600\n",
      "NCI1\t4002\t4110\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "# NB : if n_iter has been changed from 10 in the previous cell, the kernel matrices need to be recomputed with n_iter=10 here !\n",
    "print(\"Dataset\\tRank\\t# samples\")\n",
    "print(\"\\t\".join((\"MUTAG\", str(matrix_rank(K_MUT)),  str(len(G_MUT)))))\n",
    "print(\"\\t\".join((\"ENZYMES\", str(matrix_rank(K_ENZ)),  str(len(G_ENZ)))))\n",
    "print(\"\\t\".join((\"NCI1\", str(matrix_rank(K_NCI)),  str(len(G_NCI)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1. Kernel centralization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4a360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering the kernel\n",
    "ONE = np.ones((len(K_MUT),len(K_MUT)))/len(K_MUT)\n",
    "K_MUT_C = K_MUT - ONE@K_MUT - K_MUT@ONE + ONE@K_MUT@ONE\n",
    "\n",
    "ONE = np.ones((len(K_ENZ),len(K_ENZ)))/len(K_ENZ)\n",
    "K_ENZ_C = K_ENZ - ONE@K_ENZ - K_ENZ@ONE + ONE@K_ENZ@ONE\n",
    "\n",
    "ONE = np.ones((len(K_NCI),len(K_NCI)))/len(K_NCI)\n",
    "K_NCI_C = K_NCI - ONE@K_NCI - K_NCI@ONE + ONE@K_NCI@ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2. kernel-PCA implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66660661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpca(K,n_comp):\n",
    "    lambdas,alpha = np.linalg.eig(K)\n",
    "    alpha = alpha.T\n",
    "    alpha = np.array([v/np.linalg.norm(v) for v in alpha])   # normalization of the eigenvectors\n",
    "\n",
    "    tab = lambdas > 1e-11\n",
    "    new_alpha = []\n",
    "    for i in range(len(alpha)):\n",
    "        if tab[i] :\n",
    "            new_alpha.append(alpha[i])   # only consider the non-zero eigenvalues of K\n",
    "\n",
    "    alpha = np.array(new_alpha)\n",
    "\n",
    "    lambdas = np.extract(lambdas > 1e-11, lambdas)\n",
    "\n",
    "\n",
    "    dico = {}\n",
    "    for i in range(len(lambdas)):\n",
    "        dico[lambdas[i]] = alpha[i]   # map non-zero eigenvalues to their normalized eigenvector\n",
    "\n",
    "    new_dico = {}\n",
    "    for k in sorted(dico.keys(),reverse=True):   # consider the eigenvalues in decreasing order\n",
    "        new_dico[k] = dico[k]\n",
    "\n",
    "\n",
    "    K_TF = []\n",
    "    for j in range(len(K)):\n",
    "        new_coord = []   # compute the coordinates of the points of the dataset in the embedding defined by the n_comp PC's\n",
    "        for k in range(n_comp):\n",
    "            somme = 0\n",
    "            for i in range(len(alpha[k])):\n",
    "                somme += alpha[k][i].real*K[i][j]\n",
    "            new_coord.append(somme)\n",
    "        K_TF.append(np.array(new_coord))\n",
    "\n",
    "    K_TF = np.array(K_TF)\n",
    "\n",
    "    return K_TF"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3. kernel-PCA visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3434034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell may require nearly 2 minutes to be executed because NCI1 dataset is big\n",
    "\n",
    "K_MUT_TF = kpca(K_MUT_C,2)\n",
    "K_ENZ_TF = kpca(K_ENZ_C,2)\n",
    "K_NCI_TF = kpca(K_NCI_C,2)\n",
    "\n",
    "\n",
    "def visualize(K_C, K_TF, y, title):\n",
    "    tf = KernelPCA(n_components=2,kernel='linear')\n",
    "    X_tf = tf.fit_transform(K_C)\n",
    "    fig, (my_kpca_ax, kpca_ax) = plt.subplots(\n",
    "        ncols=2, figsize=(14, 4)\n",
    "    )\n",
    "    my_kpca_ax.scatter(K_TF[:,0],K_TF[:,1],c=y)\n",
    "    my_kpca_ax.set_xlabel(\"1st kPCA component\")\n",
    "    my_kpca_ax.set_ylabel(\"2nd kPCA component\")\n",
    "    my_kpca_ax.set_title(\"Our kPCA implementation \" + title)\n",
    "    \n",
    "\n",
    "    kpca_ax.scatter(X_tf[:,0],X_tf[:,1],c=y)\n",
    "    kpca_ax.set_xlabel(\"1st kPCA component\")\n",
    "    kpca_ax.set_ylabel(\"2nd kPCA component\")\n",
    "    kpca_ax.set_title(\"Sklearn's kPCA implementation \" + title)\n",
    "    savepath = \"kpca-{}.pdf\".format(title)\n",
    "    plt.savefig(savepath)\n",
    "\n",
    "visualize(K_MUT_C,K_MUT_TF,y_MUT,\"MUTAG\")\n",
    "visualize(K_ENZ_C,K_ENZ_TF,y_ENZ,\"ENZYMES\")\n",
    "visualize(K_NCI_C,K_NCI_TF,y_NCI,\"NCI1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.4. Distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56ca3926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pairwise distance for the MUTAG graph\n",
      "22.7449724619337\n",
      "Mean pairwise distance for the ENZYMES graph\n",
      "35.58220470465758\n",
      "Mean pairwise distance for the NCI1 graph\n",
      "33.46885291094932\n"
     ]
    }
   ],
   "source": [
    "def compute_dist(K):\n",
    "    dist = []\n",
    "    for i in range(len(K)):\n",
    "        for j in range(i+1,len(K)):\n",
    "            dist.append(np.sqrt(K[i][i] + K[j][j] - 2*K[i][j]))\n",
    "    return dist\n",
    "\n",
    "dist_MUT = compute_dist(K_MUT)\n",
    "dist_ENZ = compute_dist(K_ENZ)\n",
    "dist_NCI = compute_dist(K_NCI)\n",
    "\n",
    "for dataset_name, dist_matrix in ((\"MUTAG\", dist_MUT), (\"ENZYMES\", dist_ENZ), (\"NCI1\", dist_NCI)):\n",
    "    print(f\"Mean pairwise distance for the {dataset_name} graph\")\n",
    "    print(np.mean(dist_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.5. tSNE\n",
    "See report."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.6. Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_kpca_tsne(K_C,y,title):\n",
    "    tf1 = KernelPCA(n_components=2,kernel='linear')\n",
    "    tf2 = TSNE()   # by default, tSNE uses Euclidean distance as metric => ok since our distance corresponds to the Euclidean distance in the embedding space\n",
    "    X_tf_kpca = tf1.fit_transform(K_C)\n",
    "    X_tf_tsne = tf2.fit_transform(K_C)\n",
    "    fig, (kpca_ax, tsne_ax) = plt.subplots(\n",
    "        ncols=2, figsize=(14, 4)\n",
    "    )\n",
    "    kpca_ax.scatter(X_tf_kpca[:,0],X_tf_kpca[:,1],c=y)\n",
    "    kpca_ax.set_xlabel(\"1st kPCA component\")\n",
    "    kpca_ax.set_ylabel(\"2nd kPCA component\")\n",
    "    kpca_ax.set_title(\"Visualisation with kPCA \" + title)\n",
    "\n",
    "    tsne_ax.scatter(X_tf_tsne[:,0],X_tf_tsne[:,1],c=y)\n",
    "    tsne_ax.set_xlabel(\"1st dimension\")\n",
    "    tsne_ax.set_ylabel(\"2nd dimension\")\n",
    "    tsne_ax.set_title(\"Visualisation with tSNE \" + title)\n",
    "    \n",
    "    savepath = \"tsne-{}.pdf\".format(title)\n",
    "    plt.savefig(savepath)\n",
    "\n",
    "compare_kpca_tsne(K_MUT_C,y_MUT,\"MUTAG\")\n",
    "compare_kpca_tsne(K_ENZ_C,y_ENZ,\"ENZYMES\")\n",
    "compare_kpca_tsne(K_NCI_C,y_NCI,\"NCI1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fb62f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurences of the labels in MUTAG : {1: 125, -1: 63}\n"
     ]
    }
   ],
   "source": [
    "labels_MUT = {item:list(y_MUT).count(item) for item in y_MUT}\n",
    "print(\"Number of occurrences of the labels in MUTAG : \" + str(labels_MUT))\n",
    "labels_ENZ = {item:list(y_ENZ).count(item) for item in y_ENZ}\n",
    "print(\"Number of occurrences of the labels in ENZYMES : \" + str(labels_ENZ))\n",
    "labels_NCI = {item:list(y_NCI).count(item) for item in y_MUT}\n",
    "print(\"Number of occurrences of the labels in MUTAG : \" + str(labels_MUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1. A simple baseline\n",
    "The best constant model will learn what's the mode of the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTAG \t Counter({1: 125, -1: 63}) \tAccuracy of constant model\t 0.665\n",
      "ENZYMES \t Counter({6: 100, 5: 100, 1: 100, 2: 100, 3: 100, 4: 100}) \tAccuracy of constant model\t 0.167\n",
      "NCI1 \t Counter({1: 2057, 0: 2053}) \tAccuracy of constant model\t 0.5\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for dataset_name, target_dataset in ((\"MUTAG\", y_MUT), (\"ENZYMES\", y_ENZ), (\"NCI1\", y_NCI)):\n",
    "    target_counter = Counter(target_dataset)\n",
    "    print(dataset_name, \"\\t\", target_counter, '\\tAccuracy of constant model\\t', round(max(target_counter.values())/sum(target_counter.values()), 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.2. Support Vector Machines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d74e1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\tAccuracy\n",
      "MUTAG\t 0.868421052631579\n",
      "ENZYMES\t 0.43333333333333335\n",
      "NCI1\t 0.7384428223844283\n"
     ]
    }
   ],
   "source": [
    "def model_SVC(dataset,y,c,h) -> float:\n",
    "    WL = WeisfeilerLehman(n_iter=h, base_graph_kernel=VertexHistogram) \n",
    "    X = WL.fit_transform(dataset)\n",
    "\n",
    "    # Split dataset into 80/20 training/testing partitions\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = SVC(C=c)\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    accuracy = model.score(X_test,y_test)\n",
    "    return accuracy\n",
    "\n",
    "print(\"Dataset\\tAccuracy\")\n",
    "print(\"MUTAG\\t\", model_SVC(G_MUT,y_MUT,1e2,3))\n",
    "print(\"ENZYMES\\t\", model_SVC(G_ENZ,y_ENZ,1e2,3))\n",
    "print(\"NCI1\\t\", model_SVC(G_NCI,y_NCI,1e2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.3. Select hyper-parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2fdec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\tBest H\tBest C\n",
      "MUT\t 1 \t 10.0\n",
      "ENZ\t 10 \t 1000.0\n"
     ]
    }
   ],
   "source": [
    "# 2min30 to be executed\n",
    "Cs = [10**exponent for exponent in range(-5, 5)]\n",
    "Hs = range(1, 11)\n",
    "def find_best_params(data,y):\n",
    "    cv_scores = []\n",
    "    for c in Cs:\n",
    "        for h in Hs:\n",
    "            WL = WeisfeilerLehman(n_iter=h, base_graph_kernel=VertexHistogram) \n",
    "            X = WL.fit_transform(data)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            model = SVC(C=c)\n",
    "\n",
    "            scores = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\n",
    "            cv_scores.append((np.mean(scores),(c,h)))\n",
    "    return max(cv_scores, key=lambda x: x[0])\n",
    "\n",
    "scores_MUT = find_best_params(G_MUT,y_MUT)\n",
    "scores_ENZ = find_best_params(G_ENZ,y_ENZ)\n",
    "print(\"Dataset\\tBest H\\tBest C\")\n",
    "print(\"MUT\\t\", str(scores_MUT[1][1]), \"\\t\", str(scores_MUT[1][0]))\n",
    "print(\"ENZ\\t\", str(scores_ENZ[1][1]), \"\\t\", str(scores_ENZ[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a97d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\tBest H\tBest C\n",
      "NCI\t 9 \t 10000.0\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "# `find_best_params` takes too long when feeding in the 4k+ NCI data points,\n",
    "# so we need to sample a smaller portion to cut down the computation time.\n",
    "random_idx_sample = sample(range(len(y_NCI)), len(y_NCI)//10)\n",
    "sampled_G_NCI = [G_NCI[i] for i in random_idx_sample]\n",
    "sampled_y_NCI = [y_NCI[i] for i in random_idx_sample]\n",
    "scores_NCI = find_best_params(sampled_G_NCI,sampled_y_NCI)\n",
    "\n",
    "print(\"Dataset\\tBest H\\tBest C\")\n",
    "print(\"NCI\\t\", str(scores_NCI[1][1]), \"\\t\", str(scores_NCI[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\tAccuracy\n",
      "MUTAG\t 0.8947368421052632\n",
      "ENZ\t 0.5833333333333334\n",
      "NCI\t 0.8442822384428224\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset\\tAccuracy\")\n",
    "print(\"MUTAG\\t\", model_SVC(G_MUT,y_MUT, scores_MUT[1][0], scores_MUT[1][1]))\n",
    "print(\"ENZ\\t\", model_SVC(G_ENZ,y_ENZ, scores_ENZ[1][0], scores_ENZ[1][1]))\n",
    "print(\"NCI\\t\", model_SVC(G_NCI,y_NCI, scores_NCI[1][0], scores_NCI[1][1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
